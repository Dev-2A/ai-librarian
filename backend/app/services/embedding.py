import torch
import torch.nn.functional as F
from torch import Tensor
from transformers import AutoTokenizer, AutoModel
from app.core.config import get_settings

settings = get_settings()


def _last_token_pool(last_hidden_state: Tensor, attention_mask: Tensor) -> Tensor:
    """Qwen3-Embeddingì€ ë§ˆì§€ë§‰ í† í° í’€ë§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."""
    left_padding = attention_mask[:, -1].sum() == attention_mask.shape[0]
    if left_padding:
        return last_hidden_state[:, -1]
    else:
        sequence_lengths = attention_mask.sum(dim=1) - 1
        batch_size = last_hidden_state.shape[0]
        return last_hidden_state[
            torch.arange(batch_size, device=last_hidden_state.device),
            sequence_lengths,
        ]


class EmbeddingService:
    """Qwen3-Embedding ê¸°ë°˜ í…ìŠ¤íŠ¸ ì„ë² ë”© ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.device = settings.embedding_device
        self.dimension = settings.embedding_dimension
        self.model_name = settings.embedding_model_name
        
        print(f"ğŸ“¦ Loading embedding model: {self.model_name}")
        print(f"   Device: {self.device} | Dimension: {self.dimension}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModel.from_pretrained(self.model_name).to(self.device)
        self.model.eval()
        
        print("âœ… Embedding model loaded successfully")
    
    def _get_instruct(self, task: str, text: str) -> str:
        """Instruction-aware í¬ë§·: íƒœìŠ¤í¬ ì„¤ëª… + ì¿¼ë¦¬"""
        return f"Instruct: {task}\nQuery: {text}"
    
    def encode_review(self, review: str) -> list[float]:
        """
        ë„ì„œ ê°ìƒí‰ì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        - ì¿¼ë¦¬(ì¶”ì²œ ìš”ì²­) ì‹œ: instruction í¬í•¨
        """
        task = (
            "Given a book review, r etrieve books with similar themes, "
            "emotions, and reading experience"
        )
        instructed_text = self._get_instruct(task, review)
        return self._encode([instructed_text])[0]
    
    def encode_document(self, text: str) -> list[float]:
        """
        ë„ì„œ ì •ë³´(ì œëª©+ì €ì+ê°ìƒí‰)ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        - ë¬¸ì„œ ì €ì¥ ì‹œ: instruction ì—†ì´ ì¸ì½”ë”©
        """
        return self._encode([text])[0]
    
    def encode_batch(self, texts: list[str], is_query: bool = False) -> list[list[float]]:
        """ë°°ì¹˜ ì„ë² ë”©"""
        if is_query:
            task = (
                "Given a book review, retrieve books with similar themes, "
                "emotions, and reading experience"
            )
            texts = [self._get_instruct(task, t) for t in texts]
        return self._encode(texts)
    
    def _encode(self, texts: list[str]) -> list[list[float]]:
        """ë‚´ë¶€ ì¸ì½”ë”© ë¡œì§"""
        batch_dict = self.tokenizer(
            texts,
            max_length=8192,
            padding=True,
            truncation=True,
            return_tensors="pt",
            pad_to_multiple_of=8,
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.model(**batch_dict)
        
        embeddings = _last_token_pool(
            outputs.last_hidden_state, batch_dict["attention_mask"]
        )
        
        # MRL: ì§€ì •ëœ ì°¨ì›ìœ¼ë¡œ ì˜ë¼ë‚´ê¸°
        if self.dimension < embeddings.shape[-1]:
            embeddings = embeddings[:, : self.dimension]
        
        # L2 ì •ê·œí™”
        embeddings = F.normalize(embeddings, p=2, dim=1)
        
        return embeddings.cpu().tolist()


# â”€â”€ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ â”€â”€
_embedding_service: EmbeddingService | None = None


def get_embedding_service() -> EmbeddingService:
    global _embedding_service
    if _embedding_service is None:
        _embedding_service = EmbeddingService()
    return _embedding_service